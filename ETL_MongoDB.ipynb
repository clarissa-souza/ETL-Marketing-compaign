{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clarissa-souza/ETL-Marketing-compaign/blob/main/ETL_MongoDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalando e importando as bibliotecas"
      ],
      "metadata": {
        "id": "yJNgDsQZziQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqHF61pKwuV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bc71a9-f9db-4a94-e1dd-779c61b979b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 59.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=495153eb1113cffc25720bc8ad2d395d671547af9d2648eceeee3b6af3df5979\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gcsfs"
      ],
      "metadata": {
        "id": "Mp7PP-BNxPGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39138544-88a8-48f7-d378-f1f704c5e7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gcsfs\n",
            "  Downloading gcsfs-2022.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.7/dist-packages (from gcsfs) (0.4.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2.23.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.18.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gcsfs) (3.8.1)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (1.35.0)\n",
            "Requirement already satisfied: fsspec==2022.7.1 in /usr/local/lib/python3.7/dist-packages (from gcsfs) (2022.7.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (2.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (4.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (1.8.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gcsfs) (0.13.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gcsfs) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->gcsfs) (0.4.1)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.31.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (2022.2.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (1.56.4)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.17.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->gcsfs) (3.0.9)\n",
            "Installing collected packages: gcsfs\n",
            "Successfully installed gcsfs-2022.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandera"
      ],
      "metadata": {
        "id": "Rb6s-IZq0bTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35ad076-a901-4849-cb97-5dcb448d9981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandera\n",
            "  Downloading pandera-0.9.0-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pandera) (1.21.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera) (1.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pandera) (21.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from pandera) (1.3.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandera) (4.1.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from pandera) (1.9.2)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->pandera) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->pandera) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->pandera) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->pandera) (1.15.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pandera\n",
            "Successfully installed mypy-extensions-0.4.3 pandera-0.9.0 typing-inspect-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymongo[srv]"
      ],
      "metadata": {
        "id": "oShp7mWC1Lcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2079f83-161e-4d5e-eb79-efac898a8305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymongo[srv] in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0\n",
            "  Downloading dnspython-2.2.1-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: dnspython\n",
            "Successfully installed dnspython-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando o pysaprk\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# tipo structtype\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Importando para o google cloud\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "# Importando o Pandas, pandera e numpy\n",
        "import pandas as pd\n",
        "import pandera as pa\n",
        "\n",
        "# Importando o pymongo\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "\n",
        "#window function\n",
        "from pyspark.sql.window import *"
      ],
      "metadata": {
        "id": "TeLzJLy7xHsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurando o SparkSession"
      ],
      "metadata": {
        "id": "YeIFoEisF6GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#configurar a sparksession\n",
        "spark = (SparkSession.builder\n",
        "          .master('local[4]')\n",
        "          .appName('proj-individual-BC23') \n",
        "          .config('spark.ui.port','4050') \n",
        "          .getOrCreate() \n",
        "          )"
      ],
      "metadata": {
        "id": "9LobUfd4F-gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "x0rBHB4UGO1A",
        "outputId": "3493ae0b-6b67-4a83-a8af-ebbf17bb5fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fbe38494f10>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ec17d6425547:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[4]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>proj-individual-BC23</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração para acessar google cloud storage "
      ],
      "metadata": {
        "id": "FPQ-p_vhzo05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Montando o acesso ao drive para acessar a Service account\n",
        "from google.colab import drive\n",
        "drive.mount ('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZJnRDx0w5Sj",
        "outputId": "3b2ab702-75b7-43d7-f917-6be696c42c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando o caminho da Service account\n",
        "serviceAccount='/diretorio/arquivo_service_account.json'\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=serviceAccount\n",
        "\n",
        "# Instanciar o client \n",
        "client=storage.Client()\n",
        "\n",
        "# Receber o nome do bucket na variavel bucket \n",
        "bucket=client.get_bucket('bucket-proj-individual')\n",
        "\n",
        "# Escolher o arquivo dentro da bucket\n",
        "bucket.blob('marketing_campaign.csv')\n",
        "\n",
        "# Escolher uma variável que vai receber o caminho do arquivo que quero ler\n",
        "path='gs://bucket-proj-individual/marketing_campaign.csv'"
      ],
      "metadata": {
        "id": "46gREMbIxBV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuração para acessar o MongoDB"
      ],
      "metadata": {
        "id": "SphWlma-01Cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Atribui a variavel client todo o comando para ter acesso ao atlas\n",
        "client = pymongo.MongoClient ('mongodb+srv://usuario:senha@endereco_do_Mongo')\n",
        "\n",
        "# Seleciona a Database \n",
        "db = client['proj-individual']"
      ],
      "metadata": {
        "id": "A-X8egAHw57i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lendo o Dataset original da gcloud e transferindo para MongoDB na coleção original\n"
      ],
      "metadata": {
        "id": "r5H1nsmr0x2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selecionando a coleção\n",
        "colecao_original = db.original\n",
        "colecao_original.count_documents({})"
      ],
      "metadata": {
        "id": "tL2ZA_iR8ntv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando o dataset para o dataframe dp pandas\n",
        "df_panda=pd.read_csv(path)"
      ],
      "metadata": {
        "id": "lCws_sazxJ3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_panda"
      ],
      "metadata": {
        "id": "T6b54kPOxDtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerando a dict para o MongoDB\n",
        "original = df_panda.to_dict('records')"
      ],
      "metadata": {
        "id": "wMw-jjjk9SBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enviando os arquivos para o MongoDB\n",
        "colecao_original.insert_many(original)"
      ],
      "metadata": {
        "id": "7Tz2f3uWw--N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificando os arquivo no MongoDB\n",
        "colecao_original.count_documents({})"
      ],
      "metadata": {
        "id": "SGJhy4f_06gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise do dataframe no Pandas"
      ],
      "metadata": {
        "id": "cY4kpF8w-pdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)"
      ],
      "metadata": {
        "id": "McI0-TxZAVT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lendo as primeiras e últimas linhas do dataframe\n",
        "df_panda"
      ],
      "metadata": {
        "id": "Lo3SSiWu-0gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_panda.info()"
      ],
      "metadata": {
        "id": "KCeq7ux_BtFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entendendo a coluna educação => nível de educação do usuário\n",
        "sorted(pd.unique(df_panda['Education']))"
      ],
      "metadata": {
        "id": "DoiAb4xYCgOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entendendo a coluna estado civil\n",
        "sorted(pd.unique(df_panda['Marital_Status']))"
      ],
      "metadata": {
        "id": "ynLasRNHCxXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se as colunas AcceptedCmp1 é vazia\n",
        "sorted(pd.unique(df_panda['AcceptedCmp1']))"
      ],
      "metadata": {
        "id": "RtTOkL8RDAyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se as colunas AcceptedCmp2 é nula ou tem valor único\n",
        "sorted(pd.unique(df_panda['AcceptedCmp2']))"
      ],
      "metadata": {
        "id": "Oc3TP4f7EFYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se as colunas AcceptedCmp3 é nula ou tem valor único\n",
        "sorted(pd.unique(df_panda['AcceptedCmp3']))"
      ],
      "metadata": {
        "id": "x7CDity3EFIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se as colunas AcceptedCmp4 é nula ou tem valor único\n",
        "sorted(pd.unique(df_panda['AcceptedCmp4']))"
      ],
      "metadata": {
        "id": "PjRC2QgBEE8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se as colunas AcceptedCmp5 é nula ou tem valor único\n",
        "sorted(pd.unique(df_panda['AcceptedCmp5']))"
      ],
      "metadata": {
        "id": "uraDz9dqEEtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se a coluna Complain é nula ou tem valor único\n",
        "sorted(pd.unique(df_panda['Complain']))"
      ],
      "metadata": {
        "id": "NlGIdNTOJkos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se a coluna Z_CostContact é única => Sim. Então pode ser excluída\n",
        "sorted(pd.unique(df_panda['Z_CostContact']))"
      ],
      "metadata": {
        "id": "XUjgRKu4J3CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se a coluna Z_CostContact é única => Sim. Então pode ser excluída\n",
        "sorted(pd.unique(df_panda['Z_Revenue']))"
      ],
      "metadata": {
        "id": "pP10K4mLJ-wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se a coluna Response é nula ou tem valor único\n",
        "sorted(pd.unique(df_panda['Response']))"
      ],
      "metadata": {
        "id": "mdJURBnWKLdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando se as colunas AcceptedCmp1, AcceptedCmp2, AcceptedCmp3, AcceptedCmp4, AcceptedCmp5, Complain e Response  são iguais\n",
        "a=df_panda['AcceptedCmp1'].equals(df_panda['AcceptedCmp2'])\n",
        "b=df_panda['AcceptedCmp1'].equals(df_panda['AcceptedCmp3'])\n",
        "c=df_panda['AcceptedCmp1'].equals(df_panda['AcceptedCmp4'])\n",
        "d=df_panda['AcceptedCmp1'].equals(df_panda['AcceptedCmp5'])\n",
        "e=df_panda['AcceptedCmp1'].equals(df_panda['Complain'])\n",
        "f=df_panda['AcceptedCmp1'].equals(df_panda['Response'])\n",
        "g=df_panda['AcceptedCmp2'].equals(df_panda['AcceptedCmp3'])\n",
        "h=df_panda['AcceptedCmp2'].equals(df_panda['AcceptedCmp4'])\n",
        "i=df_panda['AcceptedCmp2'].equals(df_panda['AcceptedCmp5'])\n",
        "j=df_panda['AcceptedCmp2'].equals(df_panda['Complain'])\n",
        "k=df_panda['AcceptedCmp2'].equals(df_panda['Response'])\n",
        "l=df_panda['AcceptedCmp3'].equals(df_panda['AcceptedCmp4'])\n",
        "m=df_panda['AcceptedCmp3'].equals(df_panda['AcceptedCmp5'])\n",
        "n=df_panda['AcceptedCmp3'].equals(df_panda['Complain'])\n",
        "o=df_panda['AcceptedCmp3'].equals(df_panda['Response'])\n",
        "p=df_panda['AcceptedCmp4'].equals(df_panda['AcceptedCmp5'])\n",
        "q=df_panda['AcceptedCmp4'].equals(df_panda['Complain'])\n",
        "r=df_panda['AcceptedCmp4'].equals(df_panda['Response'])\n",
        "s=df_panda['AcceptedCmp5'].equals(df_panda['Complain'])\n",
        "t=df_panda['AcceptedCmp5'].equals(df_panda['Response'])\n",
        "u=df_panda['Complain'].equals(df_panda['Response'])\n",
        "\n",
        "if a==True or b==True or c==True or d==True or e==True or f==True or g==True or h==True or i==True or j==True or k==True or l==True or m==True or n==True or o==True or p==True or q==True or r==True or s==True or t==True or u==True:\n",
        "  print('Existem colunas iguais iguais ')"
      ],
      "metadata": {
        "id": "VPnOjmyXD3Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando onde estão os null's por coluna\n",
        "df_panda.isna().sum()"
      ],
      "metadata": {
        "id": "z3tCQTlLHrqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando as estatisticas do dataframe \n",
        "df_panda.describe()"
      ],
      "metadata": {
        "id": "6VegAr8PNuv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#backup dp arquivo antes do tratamento\n",
        "df_backup=df_panda.copy()"
      ],
      "metadata": {
        "id": "oA4OmSRw_3jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tratamento com Pandas"
      ],
      "metadata": {
        "id": "coHzBNl0Mmn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformando o objeto Dt_Customer em data\n",
        "df_panda['Dt_Customer'] = pd.to_datetime(df_panda['Dt_Customer'])"
      ],
      "metadata": {
        "id": "aqSyAK3o-MIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop nas colunas Z_CostContact e Z_Revenue porque o valor é o mesmo em todas as linhas, dessa forma não faz sentido ter no dataframe\n",
        "df_panda.drop(['Z_CostContact', 'Z_Revenue'],axis=1,inplace=True) "
      ],
      "metadata": {
        "id": "zSuT9AAWMpfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traduzindo as colunas \n",
        "df_panda.rename(columns={'Year_Birth':'ano_nascimento','Education':'escolaridade','Marital_Status':'estado_civil','Income':'renda_anual','Kidhome':'criancas','Teenhome':'adolescente'},inplace=True)"
      ],
      "metadata": {
        "id": "-k6FHvdbj63W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_panda.rename(columns={'NumDealsPurchases':'comp_desconto','NumWebPurchases':'comp_web','NumCatalogPurchases':'comp_catalogo','NumStorePurchases':'comp_loja','NumWebVisitsMonth':'acesso_web_ultimo_mes'},inplace=True)"
      ],
      "metadata": {
        "id": "scqP3Mz1lYsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_panda.rename(columns={'MntWines':'vl_vinho','MntFruits':'vl_fruta','MntMeatProducts':'vl_carnes','MntFishProducts':'vl_pescados','MntSweetProducts':'vl_doces','MntGoldProds':'vl_premium'},inplace=True)"
      ],
      "metadata": {
        "id": "J2EmGY_XpcrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_panda.head(3)"
      ],
      "metadata": {
        "id": "Qxx0JPJNkOMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traduzindo as linhas\n",
        "df_panda.replace(['2n Cycle', 'Basic', 'Graduation', 'Master', 'PhD'],['Especializacao','Basico','Graduacao','Mestre','PhD'],inplace=True)"
      ],
      "metadata": {
        "id": "wOVf0N2xttpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A classificação Absur, YOLO e Alone não existem\n",
        "#Absurd e YOLO são claramente classificações que não existem serão substituidos por NaN e Alone será substituido por Solteiro\n",
        "df_panda.replace(['Absurd', 'Alone','Divorced','Married','Single','Together','Widow','YOLO'],['Absurd','Solteiro','Divorciado','Casado','Solteiro','Uniao Estavel','Viuvo','YOLO'],inplace=True)"
      ],
      "metadata": {
        "id": "99aqBzPiqfZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_panda.info()"
      ],
      "metadata": {
        "id": "NJ_P6s7_49rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificando o Nan\n",
        "df_panda.isna().sum()"
      ],
      "metadata": {
        "id": "hdCCsmYbynb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtro_renda=df_panda.renda_anual.isna()"
      ],
      "metadata": {
        "id": "vVVKROurwgGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_panda.loc[filtro_renda]\n",
        "\n",
        "#Não há necessidade mudar para np.Nan porque já está em inteiro Nan"
      ],
      "metadata": {
        "id": "fyNxTJWN0-56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substituindo o Absurd e YOLO por nan porque não existe essa classificação\n",
        "df_panda.replace(['Absurd', 'YOLO'],pd.NA,inplace=True)"
      ],
      "metadata": {
        "id": "NiFUGeY2-qw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Forçando a coluna estado_civil para tipo string\n",
        "df_panda['estado_civil'] = df_panda['estado_civil'].astype(str)"
      ],
      "metadata": {
        "id": "Zs6aE8G4vH1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pd.unique(df_panda['estado_civil']))"
      ],
      "metadata": {
        "id": "lNjwYMEAtKQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificando o scheme com o Pandera"
      ],
      "metadata": {
        "id": "6k6sYfocNt97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheme = pa.DataFrameSchema(\n",
        "   columns = { \n",
        "      'ID':pa.Column(pa.Int),  \n",
        "      'ano_nascimento':pa.Column(pa.Int),    \n",
        "      'escolaridade':pa.Column(pa.String),\n",
        "      'estado_civil':pa.Column(pa.String,nullable=True),\n",
        "      'renda_anual':pa.Column(pa.Float,nullable=True),  \n",
        "      'criancas':pa.Column(pa.Int),   \n",
        "      'adolescente':pa.Column(pa.Int),   \n",
        "      'Dt_Customer':pa.Column(pa.DateTime),\n",
        "      'Recency':pa.Column(pa.Int),  \n",
        "      'vl_vinho':pa.Column(pa.Int),   \n",
        "      'vl_fruta':pa.Column(pa.Int),   \n",
        "      'vl_carnes':pa.Column(pa.Int),   \n",
        "      'vl_pescados':pa.Column(pa.Int),    \n",
        "      'vl_doces':pa.Column(pa.Int),    \n",
        "      'vl_premium':pa.Column(pa.Int),   \n",
        "      'comp_desconto':pa.Column(pa.Int),  \n",
        "      'comp_web':pa.Column(pa.Int),   \n",
        "      'comp_catalogo':pa.Column(pa.Int),   \n",
        "      'comp_loja':pa.Column(pa.Int),   \n",
        "      'acesso_web_ultimo_mes':pa.Column(pa.Int),   \n",
        "      'AcceptedCmp3':pa.Column(pa.Int),  \n",
        "      'AcceptedCmp4':pa.Column(pa.Int),    \n",
        "      'AcceptedCmp5':pa.Column(pa.Int),    \n",
        "      'AcceptedCmp1':pa.Column(pa.Int),    \n",
        "      'AcceptedCmp2':pa.Column(pa.Int),    \n",
        "      'Complain':pa.Column(pa.Int),    \n",
        "      'Response':pa.Column(pa.Int)    \n",
        "   }\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Mp8RV2fA5kFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheme.validate(df_panda)"
      ],
      "metadata": {
        "id": "d3otfZ9R7pJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Montando o esquema para o Pyspark e gerando o Dataframe pyspark"
      ],
      "metadata": {
        "id": "yqxsDAAhBVGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definindo o esquema para o PySpark renomeando as colunas Dt_custome = dt_cadastro, Recency=num_dias_ult_comp, AcceptedCmp3 = aceite_camp_3, AcceptedCmp4=aceite_camp_4, \n",
        "#AcceptedCmp5=aceite_camp_5, AcceptedCmp1=aceite_camp_1, AcceptedCmp2=aceite_camp_2, Complain = reclamacao \n",
        "\n",
        "esquema = (\n",
        "    StructType([\n",
        "      StructField('ID',IntegerType()), \n",
        "      StructField('ano_nascimento',IntegerType()),   \n",
        "      StructField('escolaridade',StringType()),\n",
        "      StructField('estado_civil',StringType()),\n",
        "      StructField('renda_anual',FloatType()), \n",
        "      StructField('criancas',IntegerType()),   \n",
        "      StructField('adolescente',IntegerType()),   \n",
        "      StructField('dt_cadastro',DateType()),\n",
        "      StructField('num_dias_ult_comp',IntegerType()),\n",
        "      StructField('vl_vinho',IntegerType()),  \n",
        "      StructField('vl_fruta',IntegerType()),   \n",
        "      StructField('vl_carnes',IntegerType()),  \n",
        "      StructField('vl_pescados',IntegerType()),   \n",
        "      StructField('vl_doces',IntegerType()), \n",
        "      StructField('vl_premium',IntegerType()),\n",
        "      StructField('comp_desconto',IntegerType()), \n",
        "      StructField('comp_web',IntegerType()), \n",
        "      StructField('comp_catalogo',IntegerType()),  \n",
        "      StructField('comp_loja',IntegerType()), \n",
        "      StructField('acesso_web_ultimo_mes',IntegerType()),   \n",
        "      StructField('aceite_camp_3',IntegerType()), \n",
        "      StructField('aceite_camp_4',IntegerType()),   \n",
        "      StructField('aceite_camp_5',IntegerType()),   \n",
        "      StructField('aceite_camp_1',IntegerType()),   \n",
        "      StructField('aceite_camp_2',IntegerType()),  \n",
        "      StructField('reclamacao',IntegerType()),  \n",
        "      StructField('Response',IntegerType()),   \n",
        "    ])\n",
        ")\n"
      ],
      "metadata": {
        "id": "vKOZWzRKHr1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gerando o Dataframe do Pyspark a partir do Dataframe do Pandas\n",
        "df_spark = spark.createDataFrame(data=df_panda, schema=esquema)"
      ],
      "metadata": {
        "id": "pDUeAjmRNMs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.show()"
      ],
      "metadata": {
        "id": "0sZsUAGe3Qna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.printSchema()"
      ],
      "metadata": {
        "id": "c7euqCyjJ6Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise do dataframe no Pyspark"
      ],
      "metadata": {
        "id": "RXLed9wFL01Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificando as primeiras e últimas linhas organizada pelo ID para ver como se comporta\n",
        "df_spark.orderBy(F.col('ID').asc()).show(10)"
      ],
      "metadata": {
        "id": "0VkRiLgSmGde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.orderBy(F.col('ID').desc()).show(10, truncate=False)"
      ],
      "metadata": {
        "id": "0WqoE3fwmQn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analisando a quantidade de linhas\n",
        "df_spark.count()"
      ],
      "metadata": {
        "id": "qRS5Ed0TNRUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analisando a quantidade de linhas distintas\n",
        "df_spark.distinct().count()"
      ],
      "metadata": {
        "id": "w16udehvLGU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificando que a coluna Response aidna precisa ser traduzida\n",
        "df_spark.show(3)"
      ],
      "metadata": {
        "id": "BozavwXr2WCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificado 02 clientes que nasceram antes de 1900. É provável que a data esteja errada mas não é possível fazer correções porque existe a possibilidade (mesmo que remota) que o cliente ainda esteja vivo.\n",
        "df_spark.orderBy(F.col('ano_nascimento').asc()).show()"
      ],
      "metadata": {
        "id": "CS9dZq0nrwPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.orderBy(F.col('estado_civil').asc()).show()"
      ],
      "metadata": {
        "id": "9wpm-VLS9tpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#backup do arquivo antes do tratamento\n",
        "df_backup=df_spark"
      ],
      "metadata": {
        "id": "va8WcsEbrc7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_backup.show()"
      ],
      "metadata": {
        "id": "Dhrx3-JLroJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tratamento com Pyspark"
      ],
      "metadata": {
        "id": "FfAH2y5kMQVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop na coluna do id porque não faz sentido no dataset\n",
        "df_spark=df_spark.drop('ID')"
      ],
      "metadata": {
        "id": "QWVOL7U0MVTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Depois de drop na coluna ID, foi possível identificar que haviam 182 linhas duplicadas. Necessário drop nas linhas duplicadas\n",
        "df_spark.count()"
      ],
      "metadata": {
        "id": "RRwgfknoxCX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.distinct().count()"
      ],
      "metadata": {
        "id": "GUsHMkNRxQXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark=df_spark.dropDuplicates()"
      ],
      "metadata": {
        "id": "qhdMWojSxdJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.count()"
      ],
      "metadata": {
        "id": "x2Pi8LP3xldt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando a coluna valor total de compra (soma de valor comprado de vinho, carne, pescado, doces e produtos premium) e a quantidade de filhos (soma de criancas e adolescentes)\n",
        "df_spark=df_spark.withColumn('filhos',F.col('criancas')+F.col('adolescente')).withColumn('vl_compras',F.col('vl_vinho')+F.col('vl_fruta')+F.col('vl_carnes')+F.col('vl_pescados')+F.col('vl_doces')+F.col('vl_premium'))"
      ],
      "metadata": {
        "id": "aJr28Shv9pQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Renomeando a coluna Response = aceite_ult_camp\n",
        "df_spark=df_spark.withColumnRenamed('Response','aceite_ult_camp')"
      ],
      "metadata": {
        "id": "ZlB7RiFTNJYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtro e Window Function PySpark"
      ],
      "metadata": {
        "id": "gLakxNPsoRFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Média da renda anual, filho e compras por escolaridade\n",
        "df_spark.fillna(value=0).groupBy('escolaridade').agg(F.round(F.mean(F.col('renda_anual')),2).alias('media renda anual'), F.round(F.mean(F.col('filhos')),2).alias('media_filhos'),F.round(F.mean(F.col('vl_compras')),2).alias('media_compras')).show()"
      ],
      "metadata": {
        "id": "dVL73feqhfxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Média da renda anual, filho e compras por estado civil\n",
        "df_spark.fillna(value=0).groupBy('estado_civil').agg(F.round(F.mean(F.col('renda_anual')),2).alias('media renda anual'), F.round(F.mean(F.col('filhos')),2).alias('media_filhos'),F.round(F.mean(F.col('vl_compras')),2).alias('media_compras')).show()"
      ],
      "metadata": {
        "id": "6XM-We0d2RUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary dos valores de compra em relação ao valor total\n",
        "filtro_vendas_escolaridade=['vl_vinho','vl_fruta','vl_carnes','vl_pescados','vl_doces','vl_premium','vl_compras']\n",
        "df_spark.select(filtro_vendas_escolaridade).summary().show()"
      ],
      "metadata": {
        "id": "r4WW4xGgNKTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.select(F.col('estado_civil'),F.col('vl_compras')).filter((F.col('escolaridade') == 'PhD') & (F.col('vl_compras') > 600)).groupBy('estado_civil').avg('vl_compras').show()"
      ],
      "metadata": {
        "id": "GEM7q2PPq_hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Função window 0\n",
        "w0=Window.partitionBy(F.col('escolaridade'),F.col('estado_civil')).orderBy(F.col('vl_compras'))\n",
        "\n",
        "#função row_number\n",
        "df_spark.select(F.col('escolaridade'),F.col('estado_civil'),F.col('renda_anual'),F.col('vl_compras')).withColumn('row_number',F.row_number().over(w0)).show()"
      ],
      "metadata": {
        "id": "_v5AhdIM53bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Função window 1\n",
        "w1=Window.partitionBy(F.col('estado_civil')).orderBy(F.col('vl_compras'))\n",
        "\n",
        "#Funcao dense_Rank\n",
        "df_spark.select(F.col('escolaridade'),F.col('estado_civil'),F.col('renda_anual'),F.col('vl_compras')).withColumn('dense_rank',F.dense_rank().over(w1)).show()"
      ],
      "metadata": {
        "id": "gwyAAtF_51td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformando em dict e enviando para o MongoDB na coleção tratada"
      ],
      "metadata": {
        "id": "35Prl6dCMV97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformando o dataframe spark para dataframe pandas\n",
        "df_pandas_tratado=df_spark.toPandas()"
      ],
      "metadata": {
        "id": "JzewG2ruiKF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trasnformando a data de datetime para strftime\n",
        "df_pandas_tratado['dt_cadastro'] = pd.to_datetime(df_pandas_tratado['dt_cadastro']).dt.strftime('%d/%m/%Y')"
      ],
      "metadata": {
        "id": "vJW_znmoigJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict=df_pandas_tratado.to_dict('records')"
      ],
      "metadata": {
        "id": "bprOQ3F1fCjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecionando a coleção\n",
        "colecao_tratada = db.tratada\n",
        "colecao_tratada.count_documents({})"
      ],
      "metadata": {
        "id": "kuJuSLyOMgK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enviando os arquivos para o MongoDB\n",
        "colecao_tratada.insert_many(df_dict)"
      ],
      "metadata": {
        "id": "EYvizRqAM0FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colecao_tratada.count_documents({})"
      ],
      "metadata": {
        "id": "eGnExwmpM75H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gráfico de auxilio na análise"
      ],
      "metadata": {
        "id": "uDc_f8nvM_dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pandas_tratado.head(3)"
      ],
      "metadata": {
        "id": "tlzi5vaANFzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark.show(3)"
      ],
      "metadata": {
        "id": "P49Qn_H0_qIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}